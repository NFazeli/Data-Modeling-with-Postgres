# Data Modeling with Postgres
## Sparkify data analysis
 *Nastaran Fazeli*

The objective of this project is to build an ETL pipeline using ***Python*** and ***PostgresSQL*** . We want to understand what songs users are listening to using an easy way to query the data, which resides in a directory of JSON logs (log_data), as well as a directory with JSON metadata (song_data) on the songs.

I to created a postgres database schema and an ETL pipeline for this analysis. 

## Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

## Log Dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)  based on the songs in the song dataset. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset were partitioned by year and month. e.g. log_data/2018/11/2018-11-12-events.json


## Star Scheme
The star scheme is a better choice since it simplifies the queries and allows faster aggregation.
The image shows the star scheme used to model the Sparkify database in the center is the ***fact table***:
* ***songplay*** - records in log data associated with song plays i.e. records with page NextSong, <br>

surounded by the ***dimention tables***:
 1. ***users*** - users in the app
 2. ***songs*** - songs in music database
 3. ***artists*** - artists in music database
 4. ***time*** - timestamps of records in ***songplays*** broken down into specific units

 ![Tux, Star_scheme](/images/Star_scheme.png)

 ## Repo files
 1. test.ipynb displays the first few rows of each table to let us check the database.
2. create_tables.py drops and creates the tables. We run this file to reset the tables before each time we run the ETL scripts.
3. etl.py reads and processes files from song_data and log_data and loads them into the tables.
4. sql_queries.py contains all the sql queries, and is imported into the last three files above.
5. README.md what you are reading now.

## Final tables (first five rows):
***users***

 ![Tux, users](/images/users.png)

***songs***

 ![Tux, songs](/images/songs.png)

***artists***

 ![Tux, artists](/images/artists.png)

***time***

 ![Tux, time](/images/time.png)

 ***songplay***

  ![Tux, songplay](/images/songplay.png)

## Conclusion

Running the etl.py pipeline provides the songplay table which is produced using the data_log dataframe and using a SELECT query we take the songId and artistId from the songs and artists tables using the condition where the song length, name and artist fron log_data matches based on song title, artist name, and song duration time in song table.

Since the data used here is a subset of the much larger dataset, the solution dataset only have 1 row with values for value containing ID for both songid and artistid in the fact table. Those are the only 2 values that the query in the sql_queries.py will return that are not-NONE. The rest of the rows will have NONE values for those two variables. 